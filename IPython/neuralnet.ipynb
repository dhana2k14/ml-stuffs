{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Origin of the Neural Networks : the perceptron\n",
    "\n",
    "###A neurology primer :\n",
    "\n",
    "A neuron is a cell which can be electrically excited. It processes and transmits information in the brain. Typically, a neuron is made of a cell body, dendrites, and an axon. Networks of neurons are interconnected in the brain by membranes called synapses. Synaptic signals from other neurons are received at the dendrites while signals to other neurons are transmitted by the axon.\n",
    "\n",
    "Perceptrons were first introduced in the 1950s and 1960s as an artificial neuron. \n",
    "\n",
    "![Fig1](Figures/perceptron.png)\n",
    "\n",
    "It is easy to see the analogy between the two : in an artificial neuron (perceptron), several inputs are combined (mimicking the role of the dendrites) to produce an output (mimicking the role of the axon).\n",
    "\n",
    "\n",
    "A simple way to combine the inputs into a single output is to compute a weighted sum. The value of this sum with respect to a given threshold gives the output.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "We an actually replace the threshold by introducing what we call a bias term :\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "This will prove handy in the following.\n",
    "\n",
    "It is quite easy to visualise the perceptron as a way of weighing information.\n",
    "Assuming we want to answer the question : will it rain tomorrow ?.\n",
    "We can gather some data about the weather today :\n",
    "- Temperature today\n",
    "- Wind direction\n",
    "- Weather in a 100 km radius\n",
    "\n",
    "The perceptron combines all this information with varying importance (the weights) and eventually fires a yes or no answer.\n",
    "\n",
    "The weights can be determined using existing data and minimising the prediction error on this data with gradient descent.\n",
    "\n",
    "Why stop at a single neuron ? After all, the brain is made up of about 100 billion neurons!\n",
    "We can combine several perceptrons together (see fig) in the hope of creating greater levels of abstractions which will allow for more complex decisions.\n",
    "\n",
    "In the following, we will now see how the weights and biases in a complicated network can be tuned to adapt automatically to any data.\n",
    "\n",
    "# 1. From the perceptron to the sigmoid neuron\n",
    "\n",
    "Recall that perceptrons take decisions on hard thresholding. This makes it difficult to tune our network. Let's give an example. Sat we want to improve the network's performance on misclassified data points. We can make a small change on one of the weight and see if classification improves. However, with hard thresholding, small changes may completely alter the behaviour of the network as the output may brutally fall on the other side of the threshold. This problem is alleviated by the use of a continuous activation function, like the sigmoid (soft thresholding).\n",
    "\n",
    "With the sigmoid, the output of the neural network becomes :\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "The differences between hard and soft thresholding are illutrated below\n",
    "\n",
    "![Hard](Figures/hard_thresh.png)\n",
    "![Soft](Figures/soft_thresh.png)\n",
    "\n",
    "In fact, any function with a shape similar to the sigmoid (a smooth transition from 0 to 1 between) would suit our need of soft thresholding. It just so happens that the sigmoid has properties which make it particularly attractive.\n",
    "\n",
    "# 2. Architecture of the neural network\n",
    "\n",
    "Now that we have the base unit of our network figured out, how do we build the full network ?\n",
    "There are arbitrarily many complicated ways of building a network (think feedback loops, arbitrary connections from one layer to another ...). We are going to focus on a subclass of networks, which will prove handy to parameterize : feedforward fully connected neural networks.\n",
    "\n",
    "Such networks are organised in layers, each unit of each layer being connected to each unit of the following layer.\n",
    "\n",
    "![Feed](Figures/nn_standard.png)\n",
    "\n",
    "\n",
    "# 3. Training the neural network : backpropagation\n",
    "\n",
    "From now on, we'll work with the MNIST data set to provide an illustration to our discussion.\n",
    "This dataset is organised in the following way: It is split into two parts. There is a training part (60 000 images, represented as 28*28 arrays with the grayscale value of the relevant pixel) and a test part (10 000 similar images). Each image corresponds to a single handwritten digit.\n",
    "\n",
    "![mnist](Figures/mnist_100_digits.png)\n",
    "\n",
    "##Architecture :\n",
    "\n",
    "We will use a simple design : 3 layers (inputs + one `hidden layer` + one output layer). \n",
    "\n",
    "One such network is shown below, with 15 hidden layer cells\n",
    "\n",
    "![archi](Figures/tikz12.png)\n",
    "\n",
    "##Notations :\n",
    "\n",
    "x is the input, a 28*28 = 784 dimensional vector\n",
    "y is the output, a 10-dimensional vector such that (1,0,0,0,0,0,0,0,0,0) represents 0, (0,1,0,0,0,0,0,0,0,0) represents 1 and so on.\n",
    "a is the 10-dimensional output of the neural network\n",
    "\n",
    "We will quantify the success of our model with the well known squared loss error :\n",
    "\n",
    "$$\\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "The reason behind this choice is that the square loss function is smooth in the parameters of the networks (i.e. its weights and biases) which makes it easier to understand how small changes in the weights and biases may affect the classification performance.\n",
    "\n",
    "##Gradient descent :\n",
    "\n",
    "In order to minimise the error $C$ identified above, we are going to use gradient descent.  \n",
    "In fact, we will make use of a slightly modified version of gradient descent to alleviate computing constraints. Indeed, vanilla gradient descent updates the gradient of $C$ by computing the derivatives with respect to all data points at each update. If we have a large training set, this may prove untractable.  \n",
    "That's why we'll implement stochastic batch gradient descent :\n",
    "- We will randomly select some data points: **a mini batch**\n",
    "- And we will compute the gradient update with respect to these points only\n",
    "- We continue sampling the training set randomly (without replacement) until it is exhausted. This completes an **epoch** of training.\n",
    "- We iterate over several epochs.\n",
    "\n",
    "The size of the batch can vary, with a small size meaning fast computations at the cost of a loss of accuracy in the computation of the gradient.\n",
    "\n",
    "Let us recapitulate : our goal is to tailor the weights $w$ and the biases $b$ of the neural network so that we minimise the cost $C$.  \n",
    "Gradient descent helps us achieve this goal by providing a way to progressively update all $w$ and $b$ to their ideal values.\n",
    "\n",
    "Mathematically :\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "  w & \\rightarrow & w' = w-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w} \\\\\n",
    "  b & \\rightarrow & b' = b-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b},\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "where the sum is on a mini-batch.\n",
    "\n",
    "##Backpropagation\n",
    "\n",
    "Let's move on to the more difficult bit.  \n",
    "The backpropagation algorithm provides us with an efficient way of computing the gradient of $C$. Let's see how.\n",
    "\n",
    "###Notation\n",
    "\n",
    "**$b^l_j$** is the bias of the jth neuron in the lth layer.  \n",
    "**$a^l_j$** is the activation of the jth neuron in the lth layer.  \n",
    "**$w^l_{jk}$** is the weight for the connection from the kth neuron in the (lâˆ’1)th layer to the jth neuron in the lth layer.\n",
    "\n",
    "Let us write down a first equation to write the output of a layer :\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\n",
    "\\end{eqnarray}\n",
    "\n",
    "This equation expresses the architecture of our network: the sum is on all the neurons of the previous layer, thereby making our network fully connected.  \n",
    "We will use the following shorthand notation as well :\n",
    "\n",
    "$$z^l_j = \\sum_k w^l_{jk} a^{l-1}_k+b^l_j$$\n",
    "\n",
    "This immediately gives : \n",
    "\n",
    "$$a^l = \\sigma(z^l)$$\n",
    "\n",
    "###Computing the derivatives of $C$ :\n",
    "\n",
    "Now that the notations are in place, recall that we want to compute the derivatives of $C$: $\\hspace{2mm}\\partial C / \\partial b^l_j$ and $\\partial C / \\partial w^l_{jk}$.\n",
    "\n",
    "\n",
    "\n",
    "#### a) Computing $\\hspace{2mm}\\partial C / \\partial b^l_j$ :\n",
    "\n",
    "Using the chain rule, we write :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial b^l_j} = \\sum_k \\frac{\\partial C}{\\partial z^l_k}\\frac{\\partial z^l_k}{\\partial b^l_j }\n",
    "\\end{equation}\n",
    "\n",
    "which reduces to :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z_l^j} \\equiv \\delta^l_j\n",
    "\\end{equation}\n",
    "\n",
    "where we have introduced the intermediate quantity $\\delta^l_j \\equiv \\frac{\\partial C}{\\partial z_l^j}$. \n",
    "\n",
    "#### b) Computing $\\delta^l_j $ :\n",
    "\n",
    "Let's compute it in two times.\n",
    "\n",
    "For the final layer $L$, using the chain rule,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C}{\\partial z^L_j} &= \\sum_i \\frac{\\partial C}{\\partial a^L_i}\\frac{\\partial a^L_i}{\\partial z^L_{j} }\\\\\n",
    "&=  \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_{j}} \\\\\n",
    "& = \\frac{\\partial C}{\\partial a^L_j} \\sigma^{\\prime}(z^L_j)\n",
    "\\end{eqnarray}\n",
    "\n",
    "For any other layer $l$, using the chain rule:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\delta^l_j & = & \\frac{\\partial C}{\\partial z^l_j}\\\\\n",
    "  & = & \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j}\\\\ \n",
    "  & = & \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k\n",
    "\\end{eqnarray}\n",
    "\n",
    "note that :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  z^{l+1}_k & = & \\sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k\\\\\n",
    "  & = & \\sum_j w^{l+1}_{kj} \\sigma(z^l_j) +b^{l+1}_k\n",
    "\\end{eqnarray}\n",
    "\n",
    "hence :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj} \\sigma'(z^l_j).\n",
    "\\end{eqnarray}\n",
    "\n",
    "thus :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\delta^l_j & = \\sum_k w^{l+1}_{kj}  \\delta^{l+1}_k \\sigma'(z^l_j) \\\\\n",
    "  \\delta^l & = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "The last line expresses the result using the Hadamard product.\n",
    "\n",
    "#### c) Computing $\\partial C / \\partial w^l_{jk}$ :\n",
    "\n",
    "All that's left is to compute $\\partial C / \\partial w^l_{jk}$.\n",
    "Once again, we make use of the chain rule :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} & = \\sum_i \\frac{\\partial C}{\\partial z^l_i}\\frac{\\partial z^l_i}{\\partial w^l_{jk} }\\\\\n",
    "& = a^{l-1}_k \\delta^l_j\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "###The backpropagation algorithm\n",
    "\n",
    "The formulas we established guide us towards the backpropagation algorithm :\n",
    "\n",
    "- Initialise by setting $a^1$ as the input data\n",
    "- Feedforward : propagate by computing $z^l$ and $a^l$ for each leayer\n",
    "- Compute $\\delta^L$.\n",
    "- Backpropagate : compute $\\delta^l$ for all remaining layers\n",
    "- Use $\\delta$ to compute $\\hspace{2mm}\\partial C / \\partial b^l_j$ and $\\partial C / \\partial w^l_{jk}$.\n",
    "\n",
    "\n",
    "# 4. Python implementation\n",
    "\n",
    "The code below is an implementation of neural networks trained by backpropagation in pure python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start by importing relevant packages :\n",
    "import numpy as np\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to unzip the mnist data and load it\n",
    "def get_mnist_data():\n",
    "    \"\"\" Unzip mnist data if needed and load it\"\"\"\n",
    "    if not os.path.isfile(\"./Data/mnist.pkl\"):\n",
    "        subprocess.call(\"gunzip -k ./Data/mnist.pkl.gz\".split(\" \"))\n",
    "    with open(\"./Data/mnist.pkl\", \"r\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Format the mnist target function\n",
    "def format_mnist(y):\n",
    "    \"\"\" Convert the 1D to 10D \"\"\"\n",
    "\n",
    "    # convert to 10 categeories\n",
    "    y_new = np.zeros((10, 1))\n",
    "    y_new[y] = 1\n",
    "    return y_new\n",
    "    \n",
    "# Let's load and format the data for our neural network\n",
    "train, test, valid = get_mnist_data()\n",
    "training_inputs = [np.reshape(x, (784, 1)) for x in train[0]]\n",
    "training_results = [format_mnist(y) for y in train[1]]\n",
    "training_data = zip(training_inputs, training_results)\n",
    "test_fm = []\n",
    "for i in range(len(test[0])):\n",
    "    test_fm.append((test[0][i], test[1][i]))\n",
    "test = test_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dimensions of our data\n",
    "print len(training_data), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network code per se\n",
    "# All documentation is inline\n",
    "\n",
    "\n",
    "class ActivationFunction(object):\n",
    "\n",
    "    \"\"\" Activation function class\n",
    "    \n",
    "    We define the activation function and its derivative.\n",
    "    Only one choice : the sigmoid.\n",
    "    Feel free to add more !\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid():\n",
    "        return lambda x: 1. / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def dsigmoid():\n",
    "        return lambda x: 1. / (1 + np.exp(-x)) * (1  - 1. / (1 + np.exp(-x)))\n",
    "\n",
    "class CostFunction(object):\n",
    "\n",
    "    \"\"\" Cost function class\n",
    "    \n",
    "    We define one function : the mse\n",
    "    Feel free to add your own (like the cross entropy)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mse():\n",
    "        return lambda y,a: 0.5 * np.power(y-a, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def dmse():\n",
    "        return lambda y,a: a - y\n",
    "\n",
    "class BasicNeuralNet(object):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Neural network class\n",
    "    Implemented in python\n",
    "    \n",
    "    Main parts :\n",
    "    \n",
    "    __init__ = initialisation of the neural networks\n",
    "    the weights and biases are randomly initialised\n",
    "    \n",
    "    _forward_pass() = operates the feedforward pass discussed above\n",
    "    _backward_pass() = operates the backpropagation pass discussed above\n",
    "    _update_gradient() = computes dCdw and dCdb for a mini batch as explained above\n",
    "    \n",
    "    fit_SGD() = fit the neural network to the data\n",
    "                it loops over epochs, create mini batches of the data\n",
    "                and minimises the cost function by gradient descent\n",
    "    score() = evaluate the classification performance on specified test_data\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            sizes,\n",
    "            lmbda = 0,\n",
    "            actfuncname=\"sigmoid\",\n",
    "            costfuncname=\"mse\",\n",
    "            verbose=False):\n",
    "        self._nlayers = len(sizes)\n",
    "        self._sizes = sizes\n",
    "        self._lmbda = lmbda\n",
    "        # Random initialisation of biases and weights.\n",
    "        #For the weights, use gaussian with std = sqrt(# of weights connecting to a neuron)\n",
    "        # So that by the CLT, their sum is gaussian with std = 1\n",
    "        # Add [0] for clearer indexing\n",
    "        self._biases = [np.array([0])] + [np.random.randn(size, 1)\n",
    "                                          for size in self._sizes[1:]]\n",
    "        self._weights = [np.array([0])] + [np.random.randn(self._sizes[i], self._sizes[i - 1])/ \\\n",
    "                                           np.sqrt(self._sizes[i-1])\n",
    "                                           for i in range(1, self._nlayers)]\n",
    "\n",
    "        # Initialisation of z\n",
    "        self._z = [np.array([0])] + [np.zeros((size, 1))\n",
    "                                     for size in self._sizes[1:]]\n",
    "\n",
    "        # Activation function\n",
    "        self._actfuncname = actfuncname\n",
    "        if self._actfuncname == \"sigmoid\":\n",
    "            self._actfunc = ActivationFunction.sigmoid()\n",
    "            self._dactfunc = ActivationFunction.dsigmoid()\n",
    "\n",
    "        # Cost function\n",
    "        self._costfuncname = costfuncname\n",
    "        if self._costfuncname == \"mse\":\n",
    "            self._costfunc = CostFunction.mse()\n",
    "            self._dcostfunc = CostFunction.dmse()\n",
    "\n",
    "\n",
    "    def _forward_pass(self, x):\n",
    "        # Initialisation of activation matrix\n",
    "        self._a = [x]\n",
    "        for layer in range(1, self._nlayers):\n",
    "            self._z[layer] = np.dot(self._weights[layer], self._a[layer-1]) \\\n",
    "                + self._biases[layer]\n",
    "            a = self._actfunc(self._z[layer])\n",
    "            self._a.append(a)\n",
    "\n",
    "        # For scoring\n",
    "        return self._a[-1]\n",
    "\n",
    "    def _backward_pass(self, y):\n",
    "        # Initialisation of error matrix\n",
    "        delta_L = self._dcostfunc(y, self._a[-1]) \\\n",
    "            * self._dactfunc(self._z[-1])\n",
    "        self._delta = [delta_L]\n",
    "        for layer in range(1, self._nlayers - 1)[::-1]:\n",
    "            delta_l = np.dot(\n",
    "                self._weights[layer + 1].T, self._delta[self._nlayers - layer -2]) \\\n",
    "                * self._dactfunc(self._z[layer])\n",
    "\n",
    "            self._delta = [delta_l] + self._delta\n",
    "        self._delta = [np.array([0])] + self._delta\n",
    "\n",
    "    def _update_gradient(self, batch, n_training):\n",
    "\n",
    "        n = len(batch)\n",
    "\n",
    "        # Initialise derivative of cost wrt bias and weights\n",
    "        dCdb = [np.array([0])] + [np.zeros((size,1)) for size in self._sizes[1:]]\n",
    "        dCdw = [np.array([0])] + [np.zeros((self._sizes[i], self._sizes[i - 1]))\n",
    "                                  for i in range(1, self._nlayers)]\n",
    "        # Loop over batch\n",
    "        for X, y in batch:\n",
    "            self._forward_pass(X)\n",
    "            self._backward_pass(y)\n",
    "\n",
    "            # Loop over layers\n",
    "            for layer in range(1, self._nlayers):\n",
    "                dCdb[layer] += self._delta[layer]/float(n)\n",
    "                dCdw[layer] += np.dot(self._delta[layer], self._a[layer - 1].T)/float(n) + self._lmbda * self._weights[layer]/float(n_training)\n",
    "\n",
    "        return dCdb, dCdw\n",
    "\n",
    "    def fit_SGD(self, training_data, learning_rate, batch_size, epochs, test_data = None):\n",
    "\n",
    "        n_samples = len(training_data)\n",
    "        # Loop over epochs\n",
    "        for ep in range(epochs):\n",
    "\n",
    "            #Shuffle data\n",
    "            np.random.shuffle(training_data)\n",
    "\n",
    "            for k in xrange(0, n_samples, batch_size):\n",
    "                batch = training_data[k:k+batch_size]\n",
    "\n",
    "                dCdb, dCdw = self._update_gradient(batch, n_samples)\n",
    "                # Update bias and weights\n",
    "                self._biases = [self._biases[layer] - learning_rate * dCdb[layer]\n",
    "                           for layer in range(self._nlayers)]\n",
    "                self._weights = [self._weights[layer] - learning_rate * dCdw[layer]\n",
    "                                 for layer in range(self._nlayers)]\n",
    "            print \"Epoch %s:\" %ep, self.score(test_data), \"/\", len(test_data)\n",
    "\n",
    "    def score(self, test_data):\n",
    "        \"\"\" Score \"\"\"\n",
    "\n",
    "        test_results = [(np.argmax(self._forward_pass(np.reshape(x, (len(x), 1)))), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the neural network\n",
    "d_NN = {\"sizes\": [784, 30, 10],\n",
    "            \"actfuncname\": \"sigmoid\",\n",
    "            \"costfuncname\": \"mse\",\n",
    "            \"batch_size\": 10,\n",
    "            \"learning_rate\": 3,\n",
    "            \"epochs\": 30,\n",
    "            \"lambda\":0,\n",
    "            \"verbose\": True}\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As advertised above, we use the sigmoid as our activation function and the mean squared error as our cost function.\n",
    "The other important parameters are :\n",
    "\n",
    "- sizes : specifies for each layer the number of unit. Here, we have 3 layers. The input layer has 784 units, the hidden layer has 30 units and the output layer has 10.\n",
    "- batch_size : The number of samples in the mini batch\n",
    "- epochs : The number of training epochs\n",
    "- lambda : The regularisation parameter (this is more advanced, stick to 0)\n",
    "- learning_rate : The learning rate in the gradient descent algorithm. This quantifies how quickly we follow the direction of increasing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9443 / 10000\n",
      "Epoch 1: 9481 / 10000\n",
      "Epoch 2: 9550 / 10000\n",
      "Epoch 3: 9546 / 10000\n",
      "Epoch 4:"
     ]
    }
   ],
   "source": [
    "# Create NN model and fit\n",
    "NeuralNet = BasicNeuralNet(\n",
    "        d_NN[\"sizes\"],\n",
    "        lmbda = d_NN[\"lambda\"],\n",
    "        actfuncname=d_NN[\"actfuncname\"],\n",
    "        costfuncname=d_NN[\"costfuncname\"],\n",
    "        verbose=d_NN[\"verbose\"])\n",
    "\n",
    "NeuralNet.fit_SGD(\n",
    "    training_data,\n",
    "    d_NN[\"learning_rate\"],\n",
    "    d_NN[\"batch_size\"],\n",
    "    d_NN[\"epochs\"],\n",
    "    test_data=test)\n",
    "\n",
    "# This may take a while to train ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play around with the parameters and see whether you can improve the classification accuracy !\n",
    "(Change number of layers, number of units, learning_rate ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
