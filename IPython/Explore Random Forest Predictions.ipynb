{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Decomposing Random Forest predictions with treeinterpreter\n",
    "\n",
    "### Lets take a sample dataset, train a random forst model and predict some values on the test set and then    decompose predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load libraries\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## import boston housing dataset and fit a random forest model\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(boston.data[:300], boston.target[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## let us pick up two arbitrary points that yields two different predictions\n",
    "instances = boston.data[[300, 309]]\n",
    "print \"Instance 0 prediction:\", rf.predict(instances[0])\n",
    "print \"Instance 1 prediction:\", rf.predict(instances[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Predictions that the random forest model made for the two data points are quite different. \n",
    "#### But why? We can now decompose the predictions into the bias term (which is just the trainset mean) and individual feature contributions, \n",
    "#### so we see which features contributed to the difference and by how much.\n",
    "#### We can simply call the treeinterpreter predict method with the model and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction, bias, contributions = ti.predict(rf, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(instances)):\n",
    "    print \"Instance 0 \", i\n",
    "    print \"Bias (trainset mean)\", bias[0]\n",
    "    print \"Feature Contributions:\"\n",
    "    for c, feature in sorted(zip(contributions[i], boston.feature_names), key = lambda x:-abs(x[0])):\n",
    "        print feature, round(c, 2)\n",
    "        print \"-\"*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### The feature contributions are sorted by their absolute impact. We can see that in the first instance \n",
    "#### where the prediction was high, most of the positive contributions came from RM, LSTAT and PTRATIO feaures. \n",
    "#### On the second instance the predicted value is much lower, since RM feature actually has a very large negative \n",
    "#### impact that is not offset by the positive impact of other features, thus taking the prediction below the dataset mean.\n",
    "###### But is the prediction actually correct? This is easy way to check: bias and contributions need to sum up to the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print prediction\n",
    "print bias + np.sum(contributions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Note that when summing up the contributions, we are dealing with floating point numbers so the values can slightly different due to rounding errors\n",
    "#### lets split the dataset into two test datasets and compute average estimated price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds1 = boston.data[300:400]\n",
    "ds2 = boston.data[400:]\n",
    "\n",
    "print np.mean(rf.predict(ds1))\n",
    "print np.mean(rf.predict(ds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### We can see that the average predicted prices for the houses in the two datasets are quite different. \n",
    "###### We can now trivially break down the contributors to this difference: which features contribute to this different and by how much.\n",
    "\n",
    "prediction1, bias1, contributions1 = ti.predict(rf, ds1)\n",
    "prediction2, bias2, contributions2 = ti.predict(rf, ds2)\n",
    "\n",
    "###### Now we can calculate mean contributions of each feature\n",
    "\n",
    "totalC1 = np.mean(contributions1, axis = 0)\n",
    "totalC2 = np.mean(contributions2, axis =0)\n",
    "\n",
    "###### Since bias are equal since the training dataset is the same, the difference to the predicted values has come from only feature\n",
    "###### contributions. In other words, the sum of feature contribution differenes should only be equal to the difference in \n",
    "###### average prediction values.\n",
    "\n",
    "print np.sum(totalC1 - totalC2)\n",
    "print np.mean(prediction1) - np.mean(prediction2)\n",
    "\n",
    "###### Finally, we can print out the differences of feature contributions in the two datasets. The sum of these is \n",
    "###### exactly the difference between the average prediction values.\n",
    "\n",
    "for c, feature in sorted(zip(totalC1 - totalC2, boston.feature_names), reverse = True):\n",
    "    print feature, round(c, 2)\n",
    "    print \"-\"*5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification trees and random forest\n",
    "#### In classification trees, the feature contribute to the estimated probabilities of a given class. We can see this on the \n",
    "#### iris dataset\n",
    "\n",
    "### load required libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "idx = range(len(iris.target))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "rf.fit(iris.data[idx][:100], iris.target[idx][:100])\n",
    "\n",
    "#### lets predict for a new single instance\n",
    "\n",
    "instance = iris.data[idx][100:101]\n",
    "print rf.predict_proba(instance)\n",
    "\n",
    "##### Breakdown of feature contributions\n",
    "\n",
    "prediction, bias, contribution = ti.predict(rf, instance)\n",
    "print \"Prediction:\", prediction\n",
    "print \"Bias (trainset mean):\", bias\n",
    "print \"Feature Contributions:\"\n",
    "for c, feature in zip(contribution[0], iris.feature_names):\n",
    "    print feature, c\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
