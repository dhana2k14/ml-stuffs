{
  "paragraphs": [
    {
      "text": "%md\n### spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n### Using spark.ml is recommended because with DataFrames the API is more versatile and flexible",
      "dateUpdated": "Dec 6, 2015 4:19:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449446999594_1252197941",
      "id": "20151206-160959_1696322151",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003espark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\u003c/h3\u003e\n\u003ch3\u003eUsing spark.ml is recommended because with DataFrames the API is more versatile and flexible\u003c/h3\u003e\n"
      },
      "dateCreated": "Dec 6, 2015 4:09:59 PM",
      "dateStarted": "Dec 6, 2015 4:13:01 PM",
      "dateFinished": "Dec 6, 2015 4:13:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### spark.ml makes it easier to combine multiple algorithms into a single pipeline, or workflow. ",
      "dateUpdated": "Dec 6, 2015 4:19:17 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449447147548_-327029373",
      "id": "20151206-161227_1217797",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003espark.ml makes it easier to combine multiple algorithms into a single pipeline, or workflow.\u003c/h3\u003e\n"
      },
      "dateCreated": "Dec 6, 2015 4:12:27 PM",
      "dateStarted": "Dec 6, 2015 4:15:53 PM",
      "dateFinished": "Dec 6, 2015 4:15:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### The key concepts introduced by the Spark ML API\n#### 1. DataFrame\n#### 2. Transformer\n#### 3. Estimator\n#### 4. Pipeline\n#### 5. Parameter",
      "dateUpdated": "Dec 6, 2015 4:19:18 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449447353058_-1034073613",
      "id": "20151206-161553_698022580",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eThe key concepts introduced by the Spark ML API\u003c/h3\u003e\n\u003ch4\u003e1. DataFrame\u003c/h4\u003e\n\u003ch4\u003e2. Transformer\u003c/h4\u003e\n\u003ch4\u003e3. Estimator\u003c/h4\u003e\n\u003ch4\u003e4. Pipeline\u003c/h4\u003e\n\u003ch4\u003e5. Parameter\u003c/h4\u003e\n"
      },
      "dateCreated": "Dec 6, 2015 4:15:53 PM",
      "dateStarted": "Dec 6, 2015 4:19:10 PM",
      "dateFinished": "Dec 6, 2015 4:19:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Example: Estimator, Transformer, and Param",
      "dateUpdated": "Dec 6, 2015 6:53:23 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449447526508_-758564397",
      "id": "20151206-161846_912246544",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eExample: Estimator, Transformer, and Param\u003c/h3\u003e\n"
      },
      "dateCreated": "Dec 6, 2015 4:18:46 PM",
      "dateStarted": "Dec 6, 2015 6:53:23 PM",
      "dateFinished": "Dec 6, 2015 6:53:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.Row\n\n// Prepare training data from a list of (label, features) tuples.\nval training \u003d sqlContext.createDataFrame(Seq(\n  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n)).toDF(\"label\", \"features\")",
      "dateUpdated": "Dec 6, 2015 6:53:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449456803341_-361942844",
      "id": "20151206-185323_1128728996",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.Row\ntraining: org.apache.spark.sql.DataFrame \u003d [label: double, features: vector]\n"
      },
      "dateCreated": "Dec 6, 2015 6:53:23 PM",
      "dateStarted": "Dec 6, 2015 6:53:46 PM",
      "dateFinished": "Dec 6, 2015 6:53:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Create a LogisticRegression instance.  This instance is an Estimator.\nval lr \u003d new LogisticRegression()",
      "dateUpdated": "Dec 6, 2015 6:54:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449456826855_-521967738",
      "id": "20151206-185346_1392268093",
      "dateCreated": "Dec 6, 2015 6:53:46 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark ML Programming Guide",
  "id": "2B7WHXHBY",
  "angularObjects": {
    "2B8PRY86Y": [],
    "2B6E7G8RC": [],
    "2B5TVWHH5": [],
    "2B87T5WMW": [],
    "2B84E6S56": [],
    "2B6NMMZHN": [],
    "2B5NP4RVA": [],
    "2B5UWQKQH": [],
    "2B7H7QK35": [],
    "2B5YJ5RSN": [],
    "2B7CQFF2V": [],
    "2B5FC3D7W": [],
    "2B7DS4VXE": []
  },
  "config": {},
  "info": {}
}