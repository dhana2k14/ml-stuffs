R and Spatial Data
========================================================

## Spatial Data in R

In any data analysis project, spatial or otherwise, it is important to have a strong 
understanding of the dataset before progressing. This section will therefore begin with 
a description of the input data used in this section. We will see how data can 
be loaded into R and exported to other formats, before going into more detail about the 
underlying structure of spatial data in R: how it 'sees' spatial data is quite unique.

### Loading spatial data in R

In most situations, the starting point of spatial analysis tasks is 
loading in pre-existing datasets. These may originate from government agencies, 
remote sensing devices or 'volunteered geographical information' from GPS devices, 
online databases such as Open Street Map or geo-tagged 
social media (Goodchild 2007).
The diversity of geographical data formats is large. 

R is able to import a very wide range of spatial data formats thanks to its
interface with the Geospatial Data Abstraction Library (GDAL), which is 
enabled by loading the package `rgdal` into R. Below we will
load data from two spatial data formats: GPS eXchange (`.gpx`)
and an ESRI Shapefile (consisting of at least files 
with `.shp`, `.shx` and `.dbf` extensions).

`readOGR` is in fact cabable of loading dozens more file formats, 
so the focus is on the *method* rather than the specific formats. 
The 'take home message' is that the `readOGR` function is capable 
of loading most common spatial file formats, but behaves differently depending on file type.
Let's start with a `.gpx` file, a tracklog recording a bicycle ride from Sheffield
to Wakefield which was uploaded Open Street Map. [!!! more detail?]

```{r Leeds to Sheffield GPS data}
# download.file("http://www.openstreetmap.org/trace/1619756/data", destfile = "data/gps-trace.gpx")
library(rgdal) # load the gdal package
ogrListLayers(dsn = "data/gps-trace.gpx") # which layers are available?
shf2lds <- readOGR(dsn = "data/gps-trace.gpx", layer = "tracks") # load track
plot(shf2lds)
shf2lds.p <- readOGR(dsn = "data/gps-trace.gpx", layer = "track_points") # load points
points(shf2lds.p[seq(1, 3000, 100),])
```

There is a lot going on in the preceding 7 lines of code, including functions that 
you are unlikely to have encountered before. Let us think about what has happened, line-by-line.

First, we used R to *download* a file from the internet, using the function `download.file`.
The two essential arguments of this function are `url` (we could have typed`url =` before the link) 
and `destfile` (which means destination file). As with any function, more optional arguments 
can be viewed by typing `?download.file`. 

When `rgdal` has succesfully loaded, the next task is not to import the file directly, 
but to find out which *layers* are available to import, with the function `ogrListLayers`.
The output from this command tells us that various layers are available, including
`tracks` and `track_points`, which we subsequently load using `readOGR`.
The basic `plot` function is used to plot the newly imported objects, ensuring they make sense.
In the second `plot` function, we take a subset of the object (see section ... for more on this).

As stated in the help documentation (accessed by entering `?readOGR`), the `dsn =` argument 
is interpreted differently depending on the type of file used. 
In the above example, the filename was the data source name. 
To load Shapefiles, by contrast, the *folder* containing the data is used:

```{r Plot of London, fig.keep='none', results='hide'}
lnd <- readOGR(dsn = "data/", "london_sport")
```

Here, the data is assumed to reside in a folder entitled `data` which in R's current 
working directory (remember to check this using `getwd()`). 
If the files were stored in the working 
directory, one would use `dsn = "."` instead. Again, it may be wise to plot the data that 
results, to ensure that it has worked correctly.
Now that the data has been loaded into R's own `sp` format, try interogating and 
plotting it, using functions such as `summary` and `plot`.

### The size of spatial datasets in R

Any data that has been read into R's *workspace*, which constitutes all 
objects that can be accessed by name and can be listed using the `ls()` function, 
can be saved in R's own data storage file type, `.RData`. Spatial datasets can get 
quite large and this can cause problems on computers by consuming all available 
random access memory (RAM) or
hard disk space available to the computer. It is therefore wise to understand 
roughly how large spatial objects are; this will also provide insight into 
how long certain functions will take to run. 

In the absence of prior knowledge, which of the two objects loaded in the 
previous section would be expected to take up more memory. One could 
hypothesise that the London boroughs represented by the object `lnd` would be
larger, but how much larger? We could simply look at the size of the associated 
files, but R also provides a function (`object.size`) for discovering how large objects loaded into
its workspace are:

```{r}
object.size(shf2lds)
object.size(lnd)
```

Surprisingly, the GPS data is larger. To see why, we can find out how many 
*vertices* (points connected by lines) are contained in each dataset:

```{r}
sapply(lnd@polygons, function(x) length(x))
x <- sapply(lnd@polygons, function(x) nrow(x@Polygons[[1]]@coords))
sum(x)

sapply(shf2lds@lines, function(x) length(x))
sapply(shf2lds@lines, function(x) nrow(x@Lines[[1]]@coords))
```

It is quite likely that the above code little sense at first; the important thing
to remember is that for each object we performed two functions: 1) a check that 
each line or polygon consists only of a single *part* (that can be joined to attribut data)
and 2) the use of `nrow` to count the number of vertices. The use of the `@` symbol should 
seem strange - its meaning will become clear in the section !!!. (Note also that the 
function `fortify`, discussed in section !!!, can also be used to extract the vertice count of 
spatial objects in R.)

Without worrying,
for now, about how these vertice counts were performed, it is clear that the GPS data 
has almost 6 times the number of vertices as does the London data, explaining its larger size.
Yet when plotted, the GPS data does not seem more detailed, implying that 
some of the vertices in the object are not needed for visualisation at the scale of 
the objects *bounding box*. 

### Simplifying geometries

The wastefulness of the GPS data for visualisation (the full dataset may
be useful for other types of analysis) raises the question following question: 
can the object be simplified such that its key features
features remain while substantially reducing its size? The answer is yes.
In the code below, we harness the 
power of the `rgeos` package and its `gSimplify` function to simplify 
spatial R objects (the code can also be used to simplify polygon geometries):

```{r fig.keep='none'}
library(rgeos)
shf2lds.simple <- gSimplify(shf2lds, tol = 0.001)
(object.size(shf2lds.simple) / object.size(shf2lds))[1]
plot(shf2lds.simple) 
plot(shf2lds, col = "red", add = T)
```

In the above block of code, `gSimplify` is given the object 
`shf2lds` and the `tol` argument, short for "tolerance",
is set at 0.001 (much larger values may be needed, for
data that use is *projected* - does not use latitude and longitude).
Comparison between the sizes of the simplified object and the orginal shows 
that the new object is less than 3% of its original size. 
Try plotting the orginal and simplified tracks on your computer:
when visualised using the `plot` function, it becomes clear that the object
`shf2lds.simple` retains the overall shape of the line and is virtually
indistinguishable from the orginal object.

This example is rather contrived because even the larger object 
`shf2lds` is only `r round(object.size(shf2lds) / 1000000, 3)[1]` Mb, 
negligible compared with the gigabytes of RAM available to modern computers. 
However, it underlines a wider point: for *visualisation* purposes at 
small spatial scales (i.e. covering a large area of the Earth on a small map), 
the *geometries* associated with spatial data can often be simplified to 
reduce processing time and usage of RAM. The other advantage of simplification 
is that it reduces the size occupied by spatial datasets when they are saved.

### Saving and exporting spatial objects




## The structure of spatial data in R

### Spatial* data

#### Points

#### Lines

#### Polygons

#### Grids and raster data

### 'Flattening' data with `fortify`

## The main spatial packages

### sp

### rgdal

### rgeos

## Maps with ggplot2

### Adding base maps with ggmap

## Manipulating spatial data

### Coordinate reference systems and transformations

### Attribute joins

### Spatial joins

A spatial join, like attribute joins, is used to transfer information from 
one dataset to another. There is a clearly defined direction to spatial joins, 
with the *target layer* receiving information from another spatial layer based on 
the proximity of elements from both layers to each other. There are three broad 
types of spatial join: one-to-one, many-to-one and one-to-many. We will focus only
the former two as the third type is rarely used.

One-to-one spatial joins are by far the easiest to understand and compute
because they simply involve the transfer of attributes in one layer to 
another, based on location. A one-to-one join is depicted in figure x below. 

```{r Illustration of a one-to-one spatial join , echo=FALSE}
plot(lnd)
points(coordinates(lnd))
```

Many-to-one spatial joins involve taking a spatial layer with many elements
and allocating the attributes associated with these elements to relatively few 
elements in the target spatial layer. A common type of many-to-one spatial join 
is the allocation of data collected at many point sources unevenly scattered over
space to polygons representing administrative boundaries, as represented in 
Fig. x.

```{r, echo=FALSE}
set.seed(5876)
```


```{r Input data for a spatial join}
lnd.stations <- readOGR("data/", "lnd-stns", p4s="+init=epsg:27700")
plot(lnd)
plot(lnd.stations[round(runif(n = 500, min = 1, max = nrow(lnd.stations))),], add = T)
```

The above code reads in a `SpatialPointsDataFrame` consisting of 2532 transport nodes
in and surrounding London and then plots a random sample of 500 of these over 
the previously loaded borough level adminsitrative boundaries. 
The reason for ploting a sample of the points rather than all of them is 
that the boundary data becomes difficult to see if all of the points are ploted. 
It is also useful to see and practice sampling techniques in practice; try to 
plot only the first 500 points, rather than a random selection, and describe the difference. 

The most obvious issue with the point data from the perspective 
of a spatial join with the 
borough data is that many of the points in the dataset are in fact located outside 
the region of interest. Thus, the first stage in the analysis is to filter the 
point data such that only those that lie within London's administrative zones are 
selected. This in itself is a kind of spatial join, and can be accomplished with the 
following code. 

```{r A spatial subset of the points}
proj4string(lnd) <- proj4string(lnd.stations)
lnd.stations <- lnd.stations[lnd, ] # select only points within lnd
plot(lnd.stations) # check the result
```

The station points now clearly follow the form of the `lnd` shape, indicating that the 
procedure worked. Let's review the code that allowed this to happen:
the first line ensured that the CRS associated with each layer is *exactly* the 
same: this step should not be required in most cases, but it is worth knowing about. 
Of course, if the coordinate systems are *actually* different in each layer, 
the function `spTransform` will be needed to make them compatible. This procedure 
is discussed in section !!!. In this case, only the name was slightly different hence 
direct alteration of the CRS name via the function `proj4string`. 

The second line of code is where the magic happens and the brilliance of 
R's sp package becomes clear: all that was needed was to place another spatial 
object in the row index of the points (`[lnd, ]`) and R automatically 
understood that a subset based on location should be produced. 
This line of code is an example of R's 'terseness' - only a single line
of code is needed to perform what is in fact quite a complex operation. 

## Spatial aggregation

Now that only stations which *intersect* with the `lnd` polygon have been selected, 
the next stage is to extract information about the points within each zone. 
This many-to-one spatial join is also known as *spatial aggregation*. 
To do this there are 
a couple of approaches: one using the `sp` package and the other using `rgeos`
(see Bivand et al. 2013, 5.3).

As with the *spatial subest* method described above, the developers of R 
have been very clever in their implementation of spatial aggregations methods.
To minimise typing and ensure consistency with R's base functions, 
`sp` extends the capabilities of the `aggregate` function 
to automatically detect whether the user is asking for a spatial or a non-spatial 
aggregation (they are, in essence, the same thing - we recommend learning 
about the non-spatial use of `aggregate` in R for comparison).

Continuing with the example of station points in London polygons, 
let us use the spatial extension of `aggregate` to count how many points are
in each borough:

```{r, fig.keep='none', results='hide'}
lndStC <- aggregate(lnd.stations, by = lnd, FUN = length)
summary(lndStC)
plot(lndStC)
```

As with the spatial subset function, the above code is extremely terse. 
The aggregate function here does three things: 1) identifies which 
stations are in which London borough; 2) uses this information to 
perform a function on the output, in this case `length`, which 
simply means "count" in this context; and 3) creates a new
spatial object equivalent to `lnd` but with updated attribute 
data to reflect the results of the spatial aggregation. The results, 
with a legend and colours added, are presented in Fig !!! below.

```{r Choropleth map of number of transport nodes in London boroughs, echo=F, eval=FALSE}
library(RColorBrewer)
brewer.pal.info
cols <- brewer.pal(4, "Greens")
# brks <- quantile(lndStC$NUMBER, type=3)
brks <- c(4, 12, 19, 29, 55)
cut(lndStC$NUMBER, brks)
gs <- cols[findInterval(lndStC$NUMBER, vec = brks)]
png(filename="figure/nStations.png", width = 600, height= 550)
plot(lndStC, col = gs)
legend("topleft", legend = levels(cut(lndStC$NUMBER, brks)), 
       fill = cols, title="N. stations")
dev.off()
```

![Number of stations in London boroughs](figure/nStations.png)


As with any spatial attribute data stored as an `sp` object,
we can look at the attributes 
of the point data using the `@` symbol:

```{r}
head(lnd.stations@data)
```

In this case we have three potentially interesting variables: 
"LEGEND", telling us what the point is, "NAME", and "MICE", 
which represents the number of mice sightings reported by the public
at that point  (this is a fictional variable).
To illustrate the power of the `aggregate` function, let us use it to 
find the average number of mices spotted in transport points in each 
London borough, and the standard deviation:

```{r, results='hide'}
lndAvMice <- aggregate(lnd.stations["MICE"], by = lnd, FUN = mean)
summary(lndAvMice)
lndSdMice <- aggregate(lnd.stations["MICE"], by = lnd, FUN = sd)
summary(lndSdMice)
```



```{r, echo=FALSE}
lnd.stations$MICE <- rpois(n = nrow(lnd.stations), lambda = 10)
```

### Clipping
